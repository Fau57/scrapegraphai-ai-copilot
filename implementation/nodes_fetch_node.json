[
    {
        "prompt": "how is fetch_node implemented in Scrapegraphai?",
        "answer": "\"\"\"\"\nFetchNode Module\n\"\"\"\n\nimport json\nfrom typing import List, Optional\n\nimport pandas as pd\nimport requests\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_core.documents import Document\n\nfrom ..docloaders import ChromiumLoader\nfrom ..utils.cleanup_html import cleanup_html\nfrom ..utils.logging import get_logger\nfrom .base_node import BaseNode\n\n\nclass FetchNode(BaseNode):\n    \"\"\"\n    A node responsible for fetching the HTML content of a specified URL and updating\n    the graph's state with this content. It uses ChromiumLoader to fetch\n    the content from a web page asynchronously (with proxy protection).\n\n    This node acts as a starting point in many scraping workflows, preparing the state\n    with the necessary HTML content for further processing by subsequent nodes in the graph.\n\n    Attributes:\n        headless (bool): A flag indicating whether the browser should run in headless mode.\n        verbose (bool): A flag indicating whether to print verbose output during execution.\n\n    Args:\n        input (str): Boolean expression defining the input keys needed from the state.\n        output (List[str]): List of output keys to be updated in the state.\n        node_config (Optional[dict]): Additional configuration for the node.\n        node_name (str): The unique identifier name for the node, defaulting to \"Fetch\".\n    \"\"\"\n\n    def __init__(\n        self,\n        input: str,\n        output: List[str],\n        node_config: Optional[dict] = None,\n        node_name: str = \"Fetch\",\n    ):\n        super().__init__(node_name, \"node\", input, output, 1, node_config)\n\n        self.headless = (\n            True if node_config is None else node_config.get(\"headless\", True)\n        )\n        self.verbose = (\n            False if node_config is None else node_config.get(\"verbose\", False)\n        )\n        self.useSoup = (\n            False if node_config is None else node_config.get(\"useSoup\", False)\n        )\n        self.loader_kwargs = (\n            {} if node_config is None else node_config.get(\"loader_kwargs\", {})\n        )\n\n    def execute(self, state):\n        \"\"\"\n        Executes the node's logic to fetch HTML content from a specified URL and\n        update the state with this content.\n\n        Args:\n            state (dict): The current state of the graph. The input keys will be used\n                            to fetch the correct data types from the state.\n\n        Returns:\n            dict: The updated state with a new output key containing the fetched HTML content.\n\n        Raises:\n            KeyError: If the input key is not found in the state, indicating that the\n                    necessary information to perform the operation is missing.\n        \"\"\"\n\n        self.logger.info(f\"--- Executing {self.node_name} Node ---\")\n\n        # Interpret input keys based on the provided input expression\n        input_keys = self.get_input_keys(state)\n        # Fetching data from the state based on the input keys\n        input_data = [state[key] for key in input_keys]\n\n        source = input_data[0]\n        if (\n            input_keys[0] == \"json_dir\"\n            or input_keys[0] == \"xml_dir\"\n            or input_keys[0] == \"csv_dir\"\n            or input_keys[0] == \"pdf_dir\"\n        ):\n            compressed_document = [\n                source\n            ]\n            \n            state.update({self.output[0]: compressed_document})\n            return state\n        # handling pdf\n        elif input_keys[0] == \"pdf\":\n            \n            # TODO: fix bytes content issue\n            loader = PyPDFLoader(source)\n            compressed_document = loader.load()\n            state.update({self.output[0]: compressed_document})\n            return state\n\n        elif input_keys[0] == \"csv\":\n            compressed_document = [\n                Document(\n                    page_content=str(pd.read_csv(source)), metadata={\"source\": \"csv\"}\n                )\n            ]\n            state.update({self.output[0]: compressed_document})\n            return state\n        elif input_keys[0] == \"json\":\n            f = open(source)\n            compressed_document = [\n                Document(page_content=str(json.load(f)), metadata={\"source\": \"json\"})\n            ]\n            state.update({self.output[0]: compressed_document})\n            return state\n\n        elif input_keys[0] == \"xml\":\n            with open(source, \"r\", encoding=\"utf-8\") as f:\n                data = f.read()\n            compressed_document = [\n                Document(page_content=data, metadata={\"source\": \"xml\"})\n            ]\n            state.update({self.output[0]: compressed_document})\n            return state\n\n        elif self.input == \"pdf_dir\":\n            pass\n\n        elif not source.startswith(\"http\"):\n            self.logger.info(f\"--- (Fetching HTML from: {source}) ---\")\n            if not source.strip():\n                raise ValueError(\"No HTML body content found in the local source.\")\n            title, minimized_body, link_urls, image_urls = cleanup_html(source, source)\n            parsed_content = f\"Title: {title}, Body: {minimized_body}, Links: {link_urls}, Images: {image_urls}\"\n            compressed_document = [\n                Document(page_content=parsed_content, metadata={\"source\": \"local_dir\"})\n            ]\n\n        elif self.useSoup:\n            self.logger.info(f\"--- (Fetching HTML from: {source}) ---\")\n            response = requests.get(source)\n            if response.status_code == 200:\n                if not response.text.strip():\n                    raise ValueError(\"No HTML body content found in the response.\")\n                title, minimized_body, link_urls, image_urls = cleanup_html(\n                    response.text, source\n                )\n                parsed_content = f\"Title: {title}, Body: {minimized_body}, Links: {link_urls}, Images: {image_urls}\"\n                compressed_document = [Document(page_content=parsed_content)]\n            else:\n                self.logger.warning(\n                    f\"Failed to retrieve contents from the webpage at url: {source}\"\n                )\n\n        else:\n            self.logger.info(f\"--- (Fetching HTML from: {source}) ---\")\n            loader_kwargs = {}\n\n            if self.node_config is not None:\n                loader_kwargs = self.node_config.get(\"loader_kwargs\", {})\n\n            loader = ChromiumLoader([source], headless=self.headless, **loader_kwargs)\n            document = loader.load()\n\n            if not document or not document[0].page_content.strip():\n                raise ValueError(\"No HTML body content found in the document fetched by ChromiumLoader.\")\n\n            title, minimized_body, link_urls, image_urls = cleanup_html(\n                str(document[0].page_content), source\n            )\n            parsed_content = f\"Title: {title}, Body: {minimized_body}, Links: {link_urls}, Images: {image_urls}\"\n\n            compressed_document = [\n                Document(page_content=parsed_content, metadata={\"source\": source})\n            ]\n\n        state.update(\n            {\n                self.output[0]: compressed_document,\n                self.output[1]: link_urls,\n                self.output[2]: image_urls,\n            }\n        )\n\n        return state\n"
    }
]