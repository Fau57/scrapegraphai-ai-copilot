[
    {
        "prompt": "how is implemented abstract_graph in Scrapegraphai?",
        "answer": "\"\"\"\nAbstractGraph Module\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Union\nimport uuid\nfrom pydantic import BaseModel\n\nfrom langchain_aws import BedrockEmbeddings\nfrom langchain_community.embeddings import HuggingFaceHubEmbeddings, OllamaEmbeddings\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\nfrom langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings\n\nfrom ..helpers import models_tokens\nfrom ..models import (\n    Anthropic,\n    AzureOpenAI,\n    Bedrock,\n    Gemini,\n    Groq,\n    HuggingFace,\n    Ollama,\n    OpenAI,\n    OneApi\n)\nfrom ..models.ernie import Ernie\nfrom ..utils.logging import set_verbosity_debug, set_verbosity_warning\n\nfrom ..helpers import models_tokens\nfrom ..models import AzureOpenAI, Bedrock, Gemini, Groq, HuggingFace, Ollama, OpenAI, Anthropic, DeepSeek\n\n\nclass AbstractGraph(ABC):\n    \"\"\"\n    Scaffolding class for creating a graph representation and executing it.\n\n        prompt (str): The prompt for the graph.\n        source (str): The source of the graph.\n        config (dict): Configuration parameters for the graph.\n        schema (str): The schema for the graph output.\n        llm_model: An instance of a language model client, configured for generating answers.\n        embedder_model: An instance of an embedding model client,\n                        configured for generating embeddings.\n        verbose (bool): A flag indicating whether to show print statements during execution.\n        headless (bool): A flag indicating whether to run the graph in headless mode.\n\n    Args:\n        prompt (str): The prompt for the graph.\n        config (dict): Configuration parameters for the graph.\n        source (str, optional): The source of the graph.\n        schema (str, optional): The schema for the graph output.\n\n    Example:\n        >>> class MyGraph(AbstractGraph):\n        ...     def _create_graph(self):\n        ...         # Implementation of graph creation here\n        ...         return graph\n        ...\n        >>> my_graph = MyGraph(\"Example Graph\", \n        {\"llm\": {\"model\": \"gpt-3.5-turbo\"}}, \"example_source\")\n        >>> result = my_graph.run()\n    \"\"\"\n\n    def __init__(self, prompt: str, config: dict, \n                 source: Optional[str] = None, schema: Optional[BaseModel] = None):\n\n        self.prompt = prompt\n        self.source = source\n        self.config = config\n        self.schema = schema\n        self.llm_model = self._create_llm(config[\"llm\"], chat=True)\n        self.embedder_model = self._create_default_embedder(llm_config=config[\"llm\"]                                                            ) if \"embeddings\" not in config else self._create_embedder(\n            config[\"embeddings\"])\n        self.verbose = False if config is None else config.get(\n            \"verbose\", False)\n        self.headless = True if config is None else config.get(\n            \"headless\", True)\n        self.loader_kwargs = config.get(\"loader_kwargs\", {})\n\n        # Create the graph\n        self.graph = self._create_graph()\n        self.final_state = None\n        self.execution_info = None\n\n        # Set common configuration parameters\n\n        verbose = bool(config and config.get(\"verbose\"))\n\n        if verbose:\n            set_verbosity_debug()\n        else:\n            set_verbosity_warning()\n\n        self.headless = True if config is None else config.get(\"headless\", True)\n        self.loader_kwargs = config.get(\"loader_kwargs\", {})\n\n        common_params = {\n            \"headless\": self.headless,\n            \"verbose\": self.verbose,\n            \"loader_kwargs\": self.loader_kwargs,\n            \"llm_model\": self.llm_model,\n            \"embedder_model\": self.embedder_model\n            }\n       \n        self.set_common_params(common_params, overwrite=False)\n\n        # set burr config\n        self.burr_kwargs = config.get(\"burr_kwargs\", None)\n        if self.burr_kwargs is not None:\n            self.graph.use_burr = True\n            if \"app_instance_id\" not in self.burr_kwargs:\n                # set a random uuid for the app_instance_id to avoid conflicts\n                self.burr_kwargs[\"app_instance_id\"] = str(uuid.uuid4())\n\n            self.graph.burr_config = self.burr_kwargs\n\n    def set_common_params(self, params: dict, overwrite=False):\n        \"\"\"\n        Pass parameters to every node in the graph unless otherwise defined in the graph.\n\n        Args:\n            params (dict): Common parameters and their values.\n        \"\"\"\n\n        for node in self.graph.nodes:\n            node.update_config(params, overwrite)\n    \n    def _create_llm(self, llm_config: dict, chat=False) -> object:\n        \"\"\"\n        Create a large language model instance based on the configuration provided.\n\n        Args:\n            llm_config (dict): Configuration parameters for the language model.\n\n        Returns:\n            object: An instance of the language model client.\n\n        Raises:\n            KeyError: If the model is not supported.\n        \"\"\"\n\n        llm_defaults = {\"temperature\": 0, \"streaming\": False}\n        llm_params = {**llm_defaults, **llm_config}\n\n        # If model instance is passed directly instead of the model details\n        if \"model_instance\" in llm_params:\n            return llm_params[\"model_instance\"]\n\n        # Instantiate the language model based on the model name\n        if \"gpt-\" in llm_params[\"model\"]:\n            try:\n                self.model_token = models_tokens[\"openai\"][llm_params[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return OpenAI(llm_params)\n        elif \"oneapi\" in llm_params[\"model\"]:\n            # take the model after the last dash\n            llm_params[\"model\"] = llm_params[\"model\"].split(\"/\")[-1]\n            try:\n                self.model_token = models_tokens[\"oneapi\"][llm_params[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model Model not supported\") from exc\n            return OneApi(llm_params)\n        elif \"azure\" in llm_params[\"model\"]:\n            # take the model after the last dash\n            llm_params[\"model\"] = llm_params[\"model\"].split(\"/\")[-1]\n            try:\n                self.model_token = models_tokens[\"azure\"][llm_params[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return AzureOpenAI(llm_params)\n\n        elif \"gemini\" in llm_params[\"model\"]:\n            try:\n                self.model_token = models_tokens[\"gemini\"][llm_params[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return Gemini(llm_params)\n        elif llm_params[\"model\"].startswith(\"claude\"):\n            try:\n                self.model_token = models_tokens[\"claude\"][llm_params[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return Anthropic(llm_params)\n        elif \"ollama\" in llm_params[\"model\"]:\n            llm_params[\"model\"] = llm_params[\"model\"].split(\"ollama/\")[-1]\n\n            # allow user to set model_tokens in config\n            try:\n                if \"model_tokens\" in llm_params:\n                    self.model_token = llm_params[\"model_tokens\"]\n                elif llm_params[\"model\"] in models_tokens[\"ollama\"]:\n                    try:\n                        self.model_token = models_tokens[\"ollama\"][llm_params[\"model\"]]\n                    except KeyError as exc:\n                        print(\"model not found, using default token size (8192)\")\n                        self.model_token = 8192\n                else:\n                    self.model_token = 8192\n            except AttributeError:\n                self.model_token = 8192\n\n            return Ollama(llm_params)\n        elif \"hugging_face\" in llm_params[\"model\"]:\n            try:\n                self.model_token = models_tokens[\"hugging_face\"][llm_params[\"model\"]]\n            except KeyError:\n                print(\"model not found, using default token size (8192)\")\n                self.model_token = 8192\n            return HuggingFace(llm_params)\n        elif \"groq\" in llm_params[\"model\"]:\n            llm_params[\"model\"] = llm_params[\"model\"].split(\"/\")[-1]\n\n            try:\n                self.model_token = models_tokens[\"groq\"][llm_params[\"model\"]]\n            except KeyError:\n                print(\"model not found, using default token size (8192)\")\n                self.model_token = 8192\n            return Groq(llm_params)\n        elif \"bedrock\" in llm_params[\"model\"]:\n            llm_params[\"model\"] = llm_params[\"model\"].split(\"/\")[-1]\n            model_id = llm_params[\"model\"]\n            client = llm_params.get(\"client\", None)\n            try:\n                self.model_token = models_tokens[\"bedrock\"][llm_params[\"model\"]]\n            except KeyError:\n                print(\"model not found, using default token size (8192)\")\n                self.model_token = 8192\n            return Bedrock(\n                {\n                    \"client\": client,\n                    \"model_id\": model_id,\n                    \"model_kwargs\": {\n                        \"temperature\": llm_params[\"temperature\"],\n                    },\n                }\n            )\n        elif \"claude-3-\" in llm_params[\"model\"]:\n            try:\n                self.model_token = models_tokens[\"claude\"][\"claude3\"]\n            except KeyError:\n                print(\"model not found, using default token size (8192)\")\n                self.model_token = 8192\n            return Anthropic(llm_params)\n        elif \"deepseek\" in llm_params[\"model\"]:\n            try:\n                self.model_token = models_tokens[\"deepseek\"][llm_params[\"model\"]]\n            except KeyError:\n                print(\"model not found, using default token size (8192)\")\n                self.model_token = 8192\n            return DeepSeek(llm_params)\n        elif \"ernie\" in llm_params[\"model\"]:\n            try:\n                self.model_token = models_tokens[\"ernie\"][llm_params[\"model\"]]\n            except KeyError:\n                print(\"model not found, using default token size (8192)\")\n                self.model_token = 8192\n            return Ernie(llm_params)\n        else:\n            raise ValueError(\"Model provided by the configuration not supported\")\n\n    def _create_default_embedder(self, llm_config=None) -> object:\n        \"\"\"\n        Create an embedding model instance based on the chosen llm model.\n\n        Returns:\n            object: An instance of the embedding model client.\n\n        Raises:\n            ValueError: If the model is not supported.\n        \"\"\"\n        if isinstance(self.llm_model, Gemini):\n            return GoogleGenerativeAIEmbeddings(\n                google_api_key=llm_config[\"api_key\"], model=\"models/embedding-001\"\n            )\n        if isinstance(self.llm_model, OpenAI):\n            return OpenAIEmbeddings(api_key=self.llm_model.openai_api_key)\n        elif isinstance(self.llm_model, AzureOpenAIEmbeddings):\n            return self.llm_model\n        elif isinstance(self.llm_model, AzureOpenAI):\n            return AzureOpenAIEmbeddings()\n        elif isinstance(self.llm_model, Ollama):\n            # unwrap the kwargs from the model whihc is a dict\n            params = self.llm_model._lc_kwargs\n            # remove streaming and temperature\n            params.pop(\"streaming\", None)\n            params.pop(\"temperature\", None)\n\n            return OllamaEmbeddings(**params)\n        elif isinstance(self.llm_model, HuggingFace):\n            return HuggingFaceHubEmbeddings(model=self.llm_model.model)\n        elif isinstance(self.llm_model, Bedrock):\n            return BedrockEmbeddings(client=None, model_id=self.llm_model.model_id)\n        else:\n            raise ValueError(\"Embedding Model missing or not supported\")\n\n    def _create_embedder(self, embedder_config: dict) -> object:\n        \"\"\"\n        Create an embedding model instance based on the configuration provided.\n\n        Args:\n            embedder_config (dict): Configuration parameters for the embedding model.\n\n        Returns:\n            object: An instance of the embedding model client.\n\n        Raises:\n            KeyError: If the model is not supported.\n        \"\"\"\n        if \"model_instance\" in embedder_config:\n            return embedder_config[\"model_instance\"]\n        # Instantiate the embedding model based on the model name\n        if \"openai\" in embedder_config[\"model\"]:\n            return OpenAIEmbeddings(api_key=embedder_config[\"api_key\"])\n        elif \"azure\" in embedder_config[\"model\"]:\n            return AzureOpenAIEmbeddings()\n        elif \"ollama\" in embedder_config[\"model\"]:\n            embedder_config[\"model\"] = embedder_config[\"model\"].split(\"ollama/\")[-1]\n            try:\n                models_tokens[\"ollama\"][embedder_config[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return OllamaEmbeddings(**embedder_config)\n        elif \"hugging_face\" in embedder_config[\"model\"]:\n            try:\n                models_tokens[\"hugging_face\"][embedder_config[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return HuggingFaceHubEmbeddings(model=embedder_config[\"model\"])\n        elif \"gemini\" in embedder_config[\"model\"]:\n            try:\n                models_tokens[\"gemini\"][embedder_config[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return GoogleGenerativeAIEmbeddings(model=embedder_config[\"model\"])\n        elif \"bedrock\" in embedder_config[\"model\"]:\n            embedder_config[\"model\"] = embedder_config[\"model\"].split(\"/\")[-1]\n            client = embedder_config.get(\"client\", None)\n            try:\n                models_tokens[\"bedrock\"][embedder_config[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return BedrockEmbeddings(client=client, model_id=embedder_config[\"model\"])\n        else:\n            raise ValueError(\"Model provided by the configuration not supported\")\n\n    def get_state(self, key=None) -> dict:\n        \"\"\" \"\"\n        Get the final state of the graph.\n\n        Args:\n            key (str, optional): The key of the final state to retrieve.\n\n        Returns:\n            dict: The final state of the graph.\n        \"\"\"\n\n        if key is not None:\n            return self.final_state[key]\n        return self.final_state\n\n    def append_node(self, node):\n        \"\"\"\n        Add a node to the graph.\n\n        Args:\n            node (BaseNode): The node to add to the graph.\n        \"\"\"\n\n        self.graph.append_node(node)\n\n    def get_execution_info(self):\n        \"\"\"\n        Returns the execution information of the graph.\n\n        Returns:\n            dict: The execution information of the graph.\n        \"\"\"\n\n        return self.execution_info\n\n    @abstractmethod\n    def _create_graph(self):\n        \"\"\"\n        Abstract method to create a graph representation.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def run(self) -> str:\n        \"\"\"\n        Abstract method to execute the graph and return the result.\n        \"\"\"\n        pass"
    }
]