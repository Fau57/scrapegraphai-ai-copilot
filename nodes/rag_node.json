[
    {
      "prompt": "What is the purpose of the RAGNode class in Scrapegraph AI?",
      "answer": "The purpose of the `RAGNode` class in Scrapegraph AI is to compress the input tokens and store the document in a vector database for retrieval. Relevant chunks are stored in the state. It allows scraping of big documents without exceeding the token limit of the language model."
    },
    {
      "prompt": "What are the attributes of the RAGNode class in Scrapegraph AI?",
      "answer": "The `RAGNode` class in Scrapegraph AI has three attributes: `llm_model`, which is an instance of a language model client configured for generating answers, `embedder_model`, which is an instance of an embedding model client configured for generating embeddings, and `verbose`, which is a boolean flag indicating whether to show print statements during execution."
    },
    {
      "prompt": "What does the execute method of the RAGNode class in Scrapegraph AI do?",
      "answer": "The execute method of the `RAGNode` class in Scrapegraph AI executes the node's logic to implement RAG (Retrieval-Augmented Generation). The method retrieves the user's prompt and the document to be compressed from the state, compresses the document using a ContextualCompressionRetriever object, and updates the state with the relevant chunks of the document."
    },
    {
      "prompt": "What happens if the input keys are not found in the state passed to the execute method of the RAGNode class in Scrapegraph AI?",
      "answer": "If the input keys are not found in the state passed to the execute method of the `RAGNode` class in Scrapegraph AI, a KeyError is raised, indicating that the necessary information for compressing the content is missing."
    },
    {
      "prompt": "What is the purpose of the FAISS class in the RAGNode class in Scrapegraph AI?",
      "answer": "The `FAISS` class is used in the `RAGNode` class in Scrapegraph AI to create a vector database for storing the compressed document. The FAISS object is created using the from_documents method and is then used as the base_retriever for the ContextualCompressionRetriever object."
    },
    {
      "prompt": "What is the implementation of the RAGNode class in Scrapegraph AI?",
      "answer": "Sure, here's the implementation of the `RAGNode` class in Scrapegraph AI:\n\n```python\n\"\"\"\nRAGNode Module\n\"\"\"\n\nfrom typing import List, Optional\n\nfrom langchain.docstore.document import Document\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import (\n    DocumentCompressorPipeline,\n    EmbeddingsFilter,\n)\nfrom langchain_community.document_transformers import EmbeddingsRedundantFilter\nfrom langchain_community.vectorstores import FAISS\n\nfrom ..utils.logging import get_logger\nfrom .base_node import BaseNode\n\n\nclass RAGNode(BaseNode):\n    \"\"\"\n    A node responsible for compressing the input tokens and storing the document\n    in a vector database for retrieval. Relevant chunks are stored in the state.\n\n    It allows scraping of big documents without exceeding the token limit of the language model.\n\n    Attributes:\n        llm_model: An instance of a language model client, configured for generating answers.\n        embedder_model: An instance of an embedding model client, configured for generating embeddings.\n        verbose (bool): A flag indicating whether to show print statements during execution.\n\n    Args:\n        input (str): Boolean expression defining the input keys needed from the state.\n        output (List[str]): List of output keys to be updated in the state.\n        node_config (dict): Additional configuration for the node.\n        node_name (str): The unique identifier name for the node, defaulting to \"Parse\".\n    \"\"\"\n\n    def __init__(\n        self,\n        input: str,\n        output: List[str],\n        node_config: Optional[dict] = None,\n        node_name: str = \"RAG\",\n    ):\n        super().__init__(node_name, \"node\", input, output, 2, node_config)\n\n        self.llm_model = node_config[\"llm_model\"]\n        self.embedder_model = node_config.get(\"embedder_model\", None)\n        self.verbose = (\n            False if node_config is None else node_config.get(\"verbose\", False)\n        )\n\n    def execute(self, state: dict) -> dict:\n        \"\"\"\n        Executes the node's logic to implement RAG (Retrieval-Augmented Generation).\n        The method updates the state with relevant chunks of the document.\n\n        Args:\n            state (dict): The current state of the graph. The input keys will be used to fetch the\n                            correct data from the state.\n\n        Returns:\n            dict: The updated state with the output key containing the relevant chunks of the document.\n\n        Raises:\n            KeyError: If the input keys are not found in the state, indicating that the\n                        necessary information for compressing the content is missing.\n        \"\"\"\n\n        self.logger.info(f\"--- Executing {self.node_name} Node ---\")\n\n        # Interpret input keys based on the provided input expression\n        input_keys = self.get_input_keys(state)\n\n        # Fetching data from the state based on the input keys\n        input_data = [state[key] for key in input_keys]\n\n        user_prompt = input_data[0]\n        doc = input_data[1]\n\n        chunked_docs = []\n\n        for i, chunk in enumerate(doc):\n            doc = Document(\n                page_content=chunk,\n                metadata={\n                    \"chunk\": i + 1,\n                },\n            )\n            chunked_docs.append(doc)\n\n        self.logger.info(\"--- (updated chunks metadata) ---\")\n\n        # check if embedder_model is provided, if not use llm_model\n        self.embedder_model = (\n            self.embedder_model if self.embedder_model else self.llm_model\n        )\n        embeddings = self.embedder_model\n\n        retriever = FAISS.from_documents(chunked_docs, embeddings).as_retriever()\n\n        redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n        # similarity_threshold could be set, now k=20\n        relevant_filter = EmbeddingsFilter(embeddings=embeddings)\n        pipeline_compressor = DocumentCompressorPipeline(\n            transformers=[redundant_filter, relevant_filter]\n        )\n        # redundant + relevant filter compressor\n        compression_retriever = ContextualCompressionRetriever(\n            base_compressor=pipeline_compressor, base_retriever=retriever\n        )\n\n        # relevant filter compressor only\n        # compression_retriever = ContextualCompressionRetriever(\n        #     base_compressor=relevant_filter, base_retriever=retriever\n        # )\n\n        compressed_docs = compression_retriever.invoke(user_prompt)\n\n        self.logger.info(\"--- (tokens compressed and vector stored) ---\")\n\n        state.update({self.output[0]: compressed_docs})\n        return state\n```"
    }
]
  