[
    {
        "prompt": "What is the fetch node in scrapegraph-ai?",
        "answer": "The `FetchNode` in ScrapeGraphAI is a node responsible for fetching the HTML content of a specified URL and updating the graph's state with this content. It uses ChromiumLoader to fetch the content from a web page asynchronously (with proxy protection).\n\nThis node acts as a starting point in many scraping workflows, preparing the statewith the necessary HTML content for further processing by subsequent nodes in the graph."
    },
    {
        "prompt": "What is the purpose of the FetchNode class in scrapegraph-ai?",
        "answer": "The `FetchNode` class is responsible for fetching the HTML content of a specified URL and updating the graph's state with this content. It uses ChromiumLoader to fetch the content from a web page asynchronously (with proxy protection)."
    },
    {
        "prompt": "What are the attributes of the FetchNode class in scrapegraph-ai?",
        "answer": "The attributes of the `FetchNode` class are `headless` (a flag indicating whether the browser should run in headless mode), verbose (a flag indicating whether to print verbose output during execution), `useSoup` (a flag indicating whether to use `BeautifulSoup` for parsing), and `loader_kwargs` (a dictionary of additional arguments to pass to the `ChromiumLoader`)."
    },
    {
        "prompt": "What are the arguments of the FetchNode class's __init__ method in scrapegraph-ai?",
        "answer": "The arguments of the `FetchNode` class's __init__ method are input (a string defining the input keys needed from the state), output (a list of output keys to be updated in the state), node_config (an optional dictionary of additional configuration for the node), and `node_name` (the unique identifier name for the node, defaulting to 'Fetch')."
    },
    {
        "prompt": "What is the purpose of the FetchNode class's execute method in scrapegraph-ai?",
        "answer": "The purpose of the `FetchNode` class's `execute` method is to fetch the HTML content from a specified URL and update the state with this content. The method first interprets the input keys based on the provided input expression and fetches the corresponding data from the state. It then uses `ChromiumLoader` to fetch the HTML content and updates the state with the new output key containing the fetched content."
    },
    {
        "prompt": "What does the FetchNode class's execute method return in scrapegraph-ai?",
        "answer": "The `FetchNode` class's `execute` method returns the updated state with the new output key containing the fetched HTML content."
    },
    {
        "prompt": "What exceptions does the FetchNode class's execute method raise in scrapegraph-ai?",
        "answer": "The `FetchNode` class's `execute` method raises a `KeyError` if the input key is not found in the state, indicating that the necessary information to perform the operation is missing."
    },
    {
        "prompt": "What is the purpose of the ChromiumLoader class in scrapegraph-ai?",
        "answer": "The `ChromiumLoader` class is used to fetch the HTML content of a specified URL asynchronously (with proxy protection). It is used by the `FetchNode` class to fetch the content from a web page."
    },
    {
        "prompt": "What is the purpose of the cleanup_html function in scrapegraph-ai?",
        "answer": "The `cleanup_html` function is used to clean up the fetched HTML content and extract the relevant information, such as the title, body, and links. It is used by the `FetchNode` class's execute method to parse the fetched HTML content."
    },
    {
        "prompt": "What is the implementation of the FetchNode class in scrapegraph-ai?",
        "answer": "Sure, here's the implementation of the `ConditionalNode` in ScrapeGraphAI:\n\n```python\n\"\"\"\"\nFetchNode Module\n\"\"\"\n\nimport json\nfrom typing import List, Optional\n\nimport pandas as pd\nimport requests\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_core.documents import Document\n\nfrom ..docloaders import ChromiumLoader\nfrom ..utils.cleanup_html import cleanup_html\nfrom ..utils.logging import get_logger\nfrom .base_node import BaseNode\n\n\nclass FetchNode(BaseNode):\n    \"\"\"\n    A node responsible for fetching the HTML content of a specified URL and updating\n    the graph's state with this content. It uses ChromiumLoader to fetch\n    the content from a web page asynchronously (with proxy protection).\n\n    This node acts as a starting point in many scraping workflows, preparing the state\n    with the necessary HTML content for further processing by subsequent nodes in the graph.\n\n    Attributes:\n        headless (bool): A flag indicating whether the browser should run in headless mode.\n        verbose (bool): A flag indicating whether to print verbose output during execution.\n\n    Args:\n        input (str): Boolean expression defining the input keys needed from the state.\n        output (List[str]): List of output keys to be updated in the state.\n        node_config (Optional[dict]): Additional configuration for the node.\n        node_name (str): The unique identifier name for the node, defaulting to \"Fetch\".\n    \"\"\"\n\n    def __init__(\n        self,\n        input: str,\n        output: List[str],\n        node_config: Optional[dict] = None,\n        node_name: str = \"Fetch\",\n    ):\n        super().__init__(node_name, \"node\", input, output, 1)\n\n        self.headless = (\n            True if node_config is None else node_config.get(\"headless\", True)\n        )\n        self.verbose = (\n            False if node_config is None else node_config.get(\"verbose\", False)\n        )\n        self.useSoup = (\n            False if node_config is None else node_config.get(\"useSoup\", False)\n        )\n        self.loader_kwargs = (\n            {} if node_config is None else node_config.get(\"loader_kwargs\", {})\n        )\n\n    def execute(self, state):\n        \"\"\"\n        Executes the node's logic to fetch HTML content from a specified URL and\n        update the state with this content.\n\n        Args:\n            state (dict): The current state of the graph. The input keys will be used\n                            to fetch the correct data types from the state.\n\n        Returns:\n            dict: The updated state with a new output key containing the fetched HTML content.\n\n        Raises:\n            KeyError: If the input key is not found in the state, indicating that the\n                    necessary information to perform the operation is missing.\n        \"\"\"\n\n        self.logger.info(f\"--- Executing {self.node_name} Node ---\")\n\n        # Interpret input keys based on the provided input expression\n        input_keys = self.get_input_keys(state)\n        # Fetching data from the state based on the input keys\n        input_data = [state[key] for key in input_keys]\n\n        source = input_data[0]\n        if (\n            input_keys[0] == \"json_dir\"\n            or input_keys[0] == \"xml_dir\"\n            or input_keys[0] == \"csv_dir\"\n            or input_keys[0] == \"pdf_dir\"\n        ):\n            compressed_document = [\n                source\n            ]\n            \n            state.update({self.output[0]: compressed_document})\n            return state\n        # handling for pdf\n        elif input_keys[0] == \"pdf\":\n            loader = PyPDFLoader(source)\n            compressed_document = loader.load()\n            state.update({self.output[0]: compressed_document})\n            return state\n\n        elif input_keys[0] == \"csv\":\n            compressed_document = [\n                Document(\n                    page_content=str(pd.read_csv(source)), metadata={\"source\": \"csv\"}\n                )\n            ]\n            state.update({self.output[0]: compressed_document})\n            return state\n        elif input_keys[0] == \"json\":\n            f = open(source)\n            compressed_document = [\n                Document(page_content=str(json.load(f)), metadata={\"source\": \"json\"})\n            ]\n            state.update({self.output[0]: compressed_document})\n            return state\n\n        elif input_keys[0] == \"xml\":\n            with open(source, \"r\", encoding=\"utf-8\") as f:\n                data = f.read()\n            compressed_document = [\n                Document(page_content=data, metadata={\"source\": \"xml\"})\n            ]\n            state.update({self.output[0]: compressed_document})\n            return state\n\n        elif self.input == \"pdf_dir\":\n            pass\n\n        elif not source.startswith(\"http\"):\n            title, minimized_body, link_urls, image_urls = cleanup_html(source, source)\n            parsed_content = f\"Title: {title}, Body: {minimized_body}, Links: {link_urls}, Images: {image_urls}\"\n            compressed_document = [\n                Document(page_content=parsed_content, metadata={\"source\": \"local_dir\"})\n            ]\n\n        elif self.useSoup:\n            response = requests.get(source)\n            if response.status_code == 200:\n                title, minimized_body, link_urls, image_urls = cleanup_html(\n                    response.text, source\n                )\n                parsed_content = f\"Title: {title}, Body: {minimized_body}, Links: {link_urls}, Images: {image_urls}\"\n                compressed_document = [Document(page_content=parsed_content)]\n            else:\n                self.logger.warning(\n                    f\"Failed to retrieve contents from the webpage at url: {source}\"\n                )\n\n        else:\n            loader_kwargs = {}\n\n            if self.node_config is not None:\n                loader_kwargs = self.node_config.get(\"loader_kwargs\", {})\n\n            loader = ChromiumLoader([source], headless=self.headless, **loader_kwargs)\n            document = loader.load()\n\n            title, minimized_body, link_urls, image_urls = cleanup_html(\n                str(document[0].page_content), source\n            )\n            parsed_content = f\"Title: {title}, Body: {minimized_body}, Links: {link_urls}, Images: {image_urls}\"\n\n            compressed_document = [\n                Document(page_content=parsed_content, metadata={\"source\": source})\n            ]\n\n        state.update(\n            {\n                self.output[0]: compressed_document,\n                self.output[1]: link_urls,\n                self.output[2]: image_urls,\n            }\n        )\n\n        return state\n```"
    }
]