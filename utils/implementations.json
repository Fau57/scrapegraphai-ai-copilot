[
    {
        "prompt": "Show me the implementation of the logging.py function in ScrapeGraphAI.",
        "answer": "```python\n\"\"\"A centralized logging system for any library\n\nsource code inspired by https://gist.github.com/DiTo97/9a0377f24236b66134eb96da1ec1693f\n\"\"\"\n\nimport logging\nimport os\nimport sys\nimport threading\nfrom functools import lru_cache\n\n\n_library_name = __name__.split(\".\", maxsplit=1)[0]\n\n_default_handler = None\n_default_logging_level = logging.WARNING\n\n_semaphore = threading.Lock()\n\n\ndef _get_library_root_logger() -> logging.Logger:\n    return logging.getLogger(_library_name)\n\n\ndef _set_library_root_logger() -> None:\n    global _default_handler\n\n    with _semaphore:\n        if _default_handler:\n            return\n\n        _default_handler = logging.StreamHandler()  # sys.stderr as stream\n\n        # https://github.com/pyinstaller/pyinstaller/issues/7334#issuecomment-1357447176\n        if sys.stderr is None:\n            sys.stderr = open(os.devnull, \"w\")\n\n        _default_handler.flush = sys.stderr.flush\n\n        library_root_logger = _get_library_root_logger()\n        library_root_logger.addHandler(_default_handler)\n        library_root_logger.setLevel(_default_logging_level)\n        library_root_logger.propagate = False\n\n\ndef get_logger(name: str | None = None) -> logging.Logger:\n    _set_library_root_logger()\n    return logging.getLogger(name or _library_name)\n\n\ndef get_verbosity() -> int:\n    _set_library_root_logger()\n    return _get_library_root_logger().getEffectiveLevel()\n\n\ndef set_verbosity(verbosity: int) -> None:\n    _set_library_root_logger()\n    _get_library_root_logger().setLevel(verbosity)\n\n\ndef set_verbosity_debug() -> None:\n    set_verbosity(logging.DEBUG)\n\n\ndef set_verbosity_info() -> None:\n    set_verbosity(logging.INFO)\n\n\ndef set_verbosity_warning() -> None:\n    set_verbosity(logging.WARNING)\n\n\ndef set_verbosity_error() -> None:\n    set_verbosity(logging.ERROR)\n\n\ndef set_verbosity_fatal() -> None:\n    set_verbosity(logging.FATAL)\n\n\ndef set_handler(handler: logging.Handler) -> None:\n    _set_library_root_logger()\n\n    assert handler is not None\n\n    _get_library_root_logger().addHandler(handler)\n\n\ndef set_default_handler() -> None:\n    set_handler(_default_handler)\n\n\ndef unset_handler(handler: logging.Handler) -> None:\n    _set_library_root_logger()\n\n    assert handler is not None\n\n    _get_library_root_logger().removeHandler(handler)\n\n\ndef unset_default_handler() -> None:\n    unset_handler(_default_handler)\n\n\ndef set_propagation() -> None:\n    _get_library_root_logger().propagate = True\n\n\ndef unset_propagation() -> None:\n    _get_library_root_logger().propagate = False\n\n\ndef set_formatting() -> None:\n    \"\"\"sets formatting for all handlers bound to the root logger\n\n    ```\n        [levelname|filename|line number] time >> message\n    ```\n    \"\"\"\n    formatter = logging.Formatter(\n        \"[%(levelname)s|%(filename)s:%(lineno)s] %(asctime)s >> %(message)s\"\n    )\n\n    for handler in _get_library_root_logger().handlers:\n        handler.setFormatter(formatter)\n\n\ndef unset_formatting() -> None:\n    for handler in _get_library_root_logger().handlers:\n        handler.setFormatter(None)\n\n\n@lru_cache(None)\ndef warning_once(self, *args, **kwargs):\n    \"\"\"emits warning logs with the same message only once\"\"\"\n    self.warning(*args, **kwargs)\n\n\nlogging.Logger.warning_once = warning_once\n\n```"
    },
    {
        "prompt": "Show me the implementation of the prettify_exec_info.py function in ScrapeGraphAI.",
        "answer": "```python\n\"\"\"\nPrettify the execution information of the graph.\n\"\"\"\n\nimport pandas as pd\n\n\ndef prettify_exec_info(complete_result: list[dict]) -> pd.DataFrame:\n    \"\"\"\n    Transforms the execution information of a graph into a DataFrame for enhanced visualization.\n\n    Args:\n        complete_result (list[dict]): The complete execution information of the graph.\n\n    Returns:\n        pd.DataFrame: A DataFrame that organizes the execution information for better readability and analysis.\n\n    Example:\n        >>> prettify_exec_info([{'node': 'A', 'status': 'success'}, {'node': 'B', 'status': 'failure'}])\n        DataFrame with columns 'node' and 'status' showing execution results for each node.\n    \"\"\"\n\n    df_nodes = pd.DataFrame(complete_result)\n\n    return df_nodes\n\n```"
    }, 
    {
        "prompt": "Show me the implementation of the token_calculator.py function in ScrapeGraphAI.",
        "answer": "```python\n\"\"\" \nModule for truncatinh in chunks the messages\n\"\"\"\nfrom typing import List\nimport tiktoken\nfrom ..helpers.models_tokens import models_tokens\n\n\ndef truncate_text_tokens(text: str, model: str, encoding_name: str) -> List[str]:\n    \"\"\"\n    Truncates text into chunks that are small enough to be processed by specified llm models.\n\n    Args:\n        text (str): The input text to be truncated.\n        model (str): The name of the llm model to determine the maximum token limit.\n        encoding_name (str): The encoding strategy used to encode the text before truncation.\n\n    Returns:\n        List[str]: A list of text chunks, each within the token limit of the specified model.\n\n    Example:\n        >>> truncate_text_tokens(\"This is a sample text for truncation.\", \"GPT-3\", \"EMBEDDING_ENCODING\")\n        [\"This is a sample text\", \"for truncation.\"]\n\n    This function ensures that each chunk of text can be tokenized by the specified model without exceeding the model's token limit.\n    \"\"\"\n\n    encoding = tiktoken.get_encoding(encoding_name)\n    max_tokens = models_tokens[model] - 500\n    encoded_text = encoding.encode(text)\n\n    chunks = [encoded_text[i:i + max_tokens]\n              for i in range(0, len(encoded_text), max_tokens)]\n\n    result = [encoding.decode(chunk) for chunk in chunks]\n\n    return result\n\n```"
    },
    {
        "prompt": "Show me the implementation of the save_audio_from_bytes.py function in ScrapeGraphAI.",
        "answer": "```python\n\"\"\"\nThis utility function saves the byte response as an audio file.\n\"\"\"\nfrom pathlib import Path\nfrom typing import Union\n\n\ndef save_audio_from_bytes(byte_response: bytes, output_path: Union[str, Path]) -> None:\n    \"\"\"\n    Saves the byte response as an audio file to the specified path.\n\n    Args:\n        byte_response (bytes): The byte array containing audio data.\n        output_path (Union[str, Path]): The destination file path where the audio file will be saved.\n\n    Example:\n        >>> save_audio_from_bytes(b'audio data', 'path/to/audio.mp3')\n\n    This function writes the byte array containing audio data to a file, saving it as an audio file.\n    \"\"\"\n\n    if not isinstance(output_path, Path):\n        output_path = Path(output_path)\n\n    with open(output_path, 'wb') as audio_file:\n        audio_file.write(byte_response)\n\n```"
    },
    {
        "prompt": "Show me the implementation of the proxy_rotation function in ScrapeGraphAI.",
        "answer": "```python\n\"\"\"\nModule for rotating proxies\n\"\"\"\n\nimport ipaddress\nimport random\nfrom typing import List, Optional, Set, TypedDict\n\nimport requests\nfrom fp.errors import FreeProxyException\nfrom fp.fp import FreeProxy\n\n\nclass ProxyBrokerCriteria(TypedDict, total=False):\n    \"\"\"proxy broker criteria\"\"\"\n\n    anonymous: bool\n    countryset: Set[str]\n    secure: bool\n    timeout: float\n    search_outside_if_empty: bool\n\n\nclass ProxySettings(TypedDict, total=False):\n    \"\"\"proxy settings\"\"\"\n\n    server: str\n    bypass: str\n    username: str\n    password: str\n\n\nclass Proxy(ProxySettings):\n    \"\"\"proxy server information\"\"\"\n\n    criteria: ProxyBrokerCriteria\n\n\ndef search_proxy_servers(\n    anonymous: bool = True,\n    countryset: Optional[Set[str]] = None,\n    secure: bool = False,\n    timeout: float = 5.0,\n    max_shape: int = 5,\n    search_outside_if_empty: bool = True,\n) -> List[str]:\n    \"\"\"search for proxy servers that match the specified broker criteria\n\n    Args:\n        anonymous: whether proxy servers should have minimum level-1 anonymity.\n        countryset: admissible proxy servers locations.\n        secure: whether proxy servers should support HTTP or HTTPS; defaults to HTTP;\n        timeout: The maximum timeout for proxy responses; defaults to 5.0 seconds.\n        max_shape: The maximum number of proxy servers to return; defaults to 5.\n        search_outside_if_empty: whether countryset should be extended if empty.\n\n    Returns:\n        A list of proxy server URLs matching the criteria.\n\n    Example:\n        >>> search_proxy_servers(\n        ...     anonymous=True,\n        ...     countryset={\"GB\", \"US\"},\n        ...     secure=True,\n        ...     timeout=1.0\n        ...     max_shape=2\n        ... )\n        [\n            \"http://103.10.63.135:8080\",\n            \"http://113.20.31.250:8080\",\n        ]\n    \"\"\"\n    proxybroker = FreeProxy(\n        anonym=anonymous,\n        country_id=countryset,\n        elite=True,\n        https=secure,\n        timeout=timeout,\n    )\n\n    def search_all(proxybroker: FreeProxy, k: int, search_outside: bool) -> List[str]:\n        candidateset = proxybroker.get_proxy_list(search_outside)\n        random.shuffle(candidateset)\n\n        positive = set()\n\n        for address in candidateset:\n            setting = {proxybroker.schema: f\"http://{address}\"}\n\n            try:\n                server = proxybroker._FreeProxy__check_if_proxy_is_working(setting)\n\n                if not server:\n                    continue\n\n                positive.add(server)\n\n                if len(positive) < k:\n                    continue\n\n                return list(positive)\n\n            except requests.exceptions.RequestException:\n                continue\n\n        n = len(positive)\n\n        if n < k and search_outside:\n            proxybroker.country_id = None\n\n            try:\n                negative = set(search_all(proxybroker, k - n, False))\n            except FreeProxyException:\n                negative = set()\n\n            positive = positive | negative\n\n        if not positive:\n            raise FreeProxyException(\"missing proxy servers for criteria\")\n\n        return list(positive)\n\n    return search_all(proxybroker, max_shape, search_outside_if_empty)\n\n\ndef _parse_proxy(proxy: ProxySettings) -> ProxySettings:\n    \"\"\"parses a proxy configuration with known server\n\n    Args:\n        proxy: The proxy configuration to parse.\n\n    Returns:\n        A 'playwright' compliant proxy configuration.\n    \"\"\"\n    assert \"server\" in proxy, \"missing server in the proxy configuration\"\n\n    auhtorization = [x in proxy for x in (\"username\", \"password\")]\n\n    message = \"username and password must be provided in pairs or not at all\"\n\n    assert all(auhtorization) or not any(auhtorization), message\n\n    parsed = {\"server\": proxy[\"server\"]}\n\n    if proxy.get(\"bypass\"):\n        parsed[\"bypass\"] = proxy[\"bypass\"]\n\n    if all(auhtorization):\n        parsed[\"username\"] = proxy[\"username\"]\n        parsed[\"password\"] = proxy[\"password\"]\n\n    return parsed\n\n\ndef _search_proxy(proxy: Proxy) -> ProxySettings:\n    \"\"\"searches for a proxy server matching the specified broker criteria\n\n    Args:\n        proxy: The proxy configuration to search for.\n\n    Returns:\n        A 'playwright' compliant proxy configuration.\n    \"\"\"\n\n\n    # remove max_shape from criteria \n    criteria = proxy.get(\"criteria\", {}).copy()\n    criteria.pop(\"max_shape\", None)\n\n    server = search_proxy_servers(max_shape=1, **criteria)[0]\n\n    return {\"server\": server}\n\n\ndef is_ipv4_address(address: str) -> bool:\n    \"\"\"If a proxy address conforms to a IPv4 address\"\"\"\n    try:\n        ipaddress.IPv4Address(address)\n        return True\n    except ipaddress.AddressValueError:\n        return False\n\n\ndef parse_or_search_proxy(proxy: Proxy) -> ProxySettings:\n    \"\"\"parses a proxy configuration or searches for a new one matching\n    the specified broker criteria\n\n    Args:\n        proxy: The proxy configuration to parse or search for.\n\n    Returns:\n        A 'playwright' compliant proxy configuration.\n\n    Notes:\n        - If the proxy server is a IP address, it is assumed to be\n        a proxy server address.\n        - If the proxy server is 'broker', a proxy server is searched for\n        based on the provided broker criteria.\n\n    Example:\n        >>> proxy = {\n        ...     \"server\": \"broker\",\n        ...     \"criteria\": {\n        ...         \"anonymous\": True,\n        ...         \"countryset\": {\"GB\", \"US\"},\n        ...         \"secure\": True,\n        ...         \"timeout\": 5.0\n        ...         \"search_outside_if_empty\": False\n        ...     }\n        ... }\n\n        >>> parse_or_search_proxy(proxy)\n        {\n            \"server\": \"<proxy-server-matching-criteria>\",\n        }\n\n    Example:\n        >>> proxy = {\n        ...     \"server\": \"192.168.1.1:8080\",\n        ...     \"username\": \"<username>\",\n        ...     \"password\": \"<password>\"\n        ... }\n\n        >>> parse_or_search_proxy(proxy)\n        {\n            \"server\": \"192.168.1.1:8080\",\n            \"username\": \"<username>\",\n            \"password\": \"<password>\"\n        }\n    \"\"\"\n    assert \"server\" in proxy, \"missing server in the proxy configuration\"\n\n    server_address = proxy[\"server\"].split(\":\", maxsplit=1)[0]\n\n    if is_ipv4_address(server_address):\n        return _parse_proxy(proxy)\n\n    assert proxy[\"server\"] == \"broker\", \"unknown proxy server\"\n\n    return _search_proxy(proxy)\n\n```"
    },
    {
        "prompt": "Show me the implementation of the parse_state_keys function in ScrapeGraphAI.",
        "answer": "```python\n\"\"\" \nParse_state_key module\n\"\"\"\nimport re\n\n\ndef parse_expression(expression, state: dict) -> list:\n    \"\"\"\n    Parses a complex boolean expression involving state keys.\n\n    Args:\n        expression (str): The boolean expression to parse.\n        state (dict): Dictionary of state keys used to evaluate the expression.\n\n    Raises:\n        ValueError: If the expression is empty, has adjacent state keys without operators, invalid operator usage,\n                    unbalanced parentheses, or if no state keys match the expression.\n\n    Returns:\n        list: A list of state keys that match the boolean expression, ensuring each key appears only once.\n\n    Example:\n        >>> parse_expression(\"user_input & (relevant_chunks | parsed_document | document)\", \n                            {\"user_input\": None, \"document\": None, \"parsed_document\": None, \"relevant_chunks\": None})\n        ['user_input', 'relevant_chunks', 'parsed_document', 'document']\n\n    This function evaluates the expression to determine the logical inclusion of state keys based on provided boolean logic.\n    It checks for syntax errors such as unbalanced parentheses, incorrect adjacency of operators, and empty expressions.\n    \"\"\"\n\n    # Check for empty expression\n    if not expression:\n        raise ValueError(\"Empty expression.\")\n\n    # Check for adjacent state keys without an operator between them\n    pattern = r'\\b(' + '|'.join(re.escape(key) for key in state.keys()) + \\\n        r')(\\b\\s*\\b)(' + '|'.join(re.escape(key)\n                                  for key in state.keys()) + r')\\b'\n    if re.search(pattern, expression):\n        raise ValueError(\n            \"Adjacent state keys found without an operator between them.\")\n\n    # Remove spaces\n    expression = expression.replace(\" \", \"\")\n\n    # Check for operators with empty adjacent tokens or at the start/end\n    if expression[0] in '&|' or expression[-1] in '&|' or \\\n        '&&' in expression or '||' in expression or \\\n            '&|' in expression or '|&' in expression:\n\n        raise ValueError(\"Invalid operator usage.\")\n\n    # Check for balanced parentheses and valid operator placement\n    open_parentheses = close_parentheses = 0\n    for i, char in enumerate(expression):\n        if char == '(':\n            open_parentheses += 1\n        elif char == ')':\n            close_parentheses += 1\n        # Check for invalid operator sequences\n        if char in \"&|\" and i + 1 < len(expression) and expression[i + 1] in \"&|\":\n            raise ValueError(\n                \"Invalid operator placement: operators cannot be adjacent.\")\n\n    # Check for missing or balanced parentheses\n    if open_parentheses != close_parentheses:\n        raise ValueError(\"Missing or unbalanced parentheses in expression.\")\n\n    # Helper function to evaluate an expression without parentheses\n    def evaluate_simple_expression(exp):\n        # Split the expression by the OR operator and process each segment\n        for or_segment in exp.split('|'):\n            # Check if all elements in an AND segment are in state\n            and_segment = or_segment.split('&')\n            if all(elem.strip() in state for elem in and_segment):\n                return [elem.strip() for elem in and_segment if elem.strip() in state]\n        return []\n\n    # Helper function to evaluate expressions with parentheses\n    def evaluate_expression(expression):\n        while '(' in expression:\n            start = expression.rfind('(')\n            end = expression.find(')', start)\n            sub_exp = expression[start + 1:end]\n            # Replace the evaluated part with a placeholder and then evaluate it\n            sub_result = evaluate_simple_expression(sub_exp)\n            # For simplicity in handling, join sub-results with OR to reprocess them later\n            expression = expression[:start] + \\\n                '|'.join(sub_result) + expression[end+1:]\n        return evaluate_simple_expression(expression)\n\n    temp_result = evaluate_expression(expression)\n\n    if not temp_result:\n        raise ValueError(\"No state keys matched the expression.\")\n\n    # Remove redundant state keys from the result, without changing their order\n    final_result = []\n    for key in temp_result:\n        if key not in final_result:\n            final_result.append(key)\n\n    return final_result\n\n\nEXPRESSION = \"user_input & (relevant_chunks | parsed_document | document)\"\nstate = {\n    \"user_input\": None,\n    \"document\": None,\n    \"parsed_document\": None,\n    \"relevant_chunks\": None,\n}\n\ntry:\n    result = parse_expression(EXPRESSION, state)\n    print(\"Matched keys:\", result)\nexcept ValueError as e:\n    print(\"Error:\", e)\n\n```"
    },
    {
        "prompt": "Show me the implementation of the convert_to_json function in ScrapeGraphAI.",
        "answer": "```python\n\"\"\"\nConvert to json module\n\"\"\"\nimport json\nimport os\nimport sys\n\n\ndef convert_to_json(data: dict, filename: str, position: str = None) -> None:\n    \"\"\"\n    Converts a dictionary to a JSON file and saves it at a specified location.\n\n    Args:\n        data (dict): The data to be converted into JSON format.\n        filename (str): The name of the output JSON file, without the '.json' extension.\n        position (str, optional): The file path where the JSON file should be saved. Defaults to the directory of the caller script if not provided.\n\n    Returns:\n        None: The function does not return anything.\n        \n    Raises:\n        ValueError: If 'filename' contains '.json'.\n        FileNotFoundError: If the specified directory does not exist.\n        PermissionError: If write permissions are lacking for the directory.\n\n    Example:\n        >>> convert_to_json({'id': [1, 2], 'value': [10, 20]}, 'output', '/path/to/save')\n        Saves a JSON file named 'output.json' at '/path/to/save'.\n\n    Notes:\n        This function automatically ensures the directory exists before attempting to write the file. If the directory does not exist, it will attempt to create it.\n    \"\"\"\n\n    if \".json\" in filename:\n        filename = filename.replace(\".json\", \"\")  # Remove .json extension\n\n  # Get the directory of the caller script\n    if position is None:\n        # Get directory of the main script\n        caller_dir = os.path.dirname(os.path.abspath(sys.argv[0]))\n        position = caller_dir\n\n    try:\n        os.makedirs(position, exist_ok=True)\n        with open(os.path.join(position, f\"{filename}.json\"), \"w\", encoding=\"utf-8\") as f:\n            f.write(json.dumps(data))\n    except FileNotFoundError as fnfe:\n        raise FileNotFoundError(\n            f\"The specified directory '{position}' does not exist.\") from fnfe\n    except PermissionError as pe:\n        raise PermissionError(\n            f\"You don't have permission to write to '{position}'.\") from pe\n\n```"
    },
    {
        "prompt": "Show me the implementation of the convert_to_csv function in ScrapeGraphAI.",
        "answer": "```python\n\"\"\"\nModule that given a filename and a position saves the file in the csv format\n\"\"\"\nimport os\nimport sys\nimport pandas as pd\n\n\ndef convert_to_csv(data: dict, filename: str, position: str = None) -> None:\n    \"\"\"\n    Converts a dictionary to a CSV file and saves it at a specified location.\n\n    Args:\n        data (dict): The data to be converted into CSV format.\n        filename (str): The name of the output CSV file, without the '.csv' extension.\n        position (str, optional): The file path where the CSV should be saved. Defaults to the directory of the caller script if not provided.\n\n    Returns:\n        None: The function does not return anything.\n        \n    Raises:\n        FileNotFoundError: If the specified directory does not exist.\n        PermissionError: If write permissions are lacking for the directory.\n        TypeError: If `data` is not a dictionary.\n        Exception: For other issues that may arise during the creation or saving of the CSV file.\n\n    Example:\n        >>> convert_to_csv({'id': [1, 2], 'value': [10, 20]}, 'output', '/path/to/save')\n        Saves a CSV file named 'output.csv' at '/path/to/save'.\n    \"\"\"\n\n    if \".csv\" in filename:\n        filename = filename.replace(\".csv\", \"\")  # Remove .csv extension\n\n    # Get the directory of the caller script if position is not provided\n    if position is None:\n        caller_dir = os.path.dirname(os.path.abspath(sys.argv[0]))\n        position = caller_dir\n\n    try:\n        if not isinstance(data, dict):\n            raise TypeError(\"Input data must be a dictionary\")\n\n        os.makedirs(position, exist_ok=True)  # Create directory if needed\n\n        df = pd.DataFrame.from_dict(data, orient='index')\n        df.to_csv(os.path.join(position, f\"{filename}.csv\"), index=False)\n\n    except FileNotFoundError as fnfe:\n        raise FileNotFoundError(\n            f\"The specified directory '{position}' does not exist.\") from fnfe\n    except PermissionError as pe:\n        raise PermissionError(\n            f\"You don't have permission to write to '{position}'.\") from pe\n    except Exception as e:\n        raise e  # Re-raise other potential errors\n\n```"
    },
    {
        "prompt": "Show me the implementation of the cleanup_html function in ScrapeGraphAI.",
        "answer": "```python\n\"\"\" \nModule for minimizing the code\n\"\"\"\nfrom bs4 import BeautifulSoup\nfrom minify_html import minify\nfrom urllib.parse import urljoin\n\n\ndef cleanup_html(html_content: str, base_url: str) -> str:\n    \"\"\"\n    Processes HTML content by removing unnecessary tags, minifying the HTML, and extracting the title and body content.\n\n    Args:\n        html_content (str): The HTML content to be processed.\n\n    Returns:\n        str: A string combining the parsed title and the minified body content. If no body content is found, it indicates so.\n\n    Example:\n        >>> html_content = \"<html><head><title>Example</title></head><body><p>Hello World!</p></body></html>\"\n        >>> remover(html_content)\n        'Title: Example, Body: <body><p>Hello World!</p></body>'\n\n    This function is particularly useful for preparing HTML content for environments where bandwidth usage needs to be minimized.\n    \"\"\"\n\n    soup = BeautifulSoup(html_content, 'html.parser')\n\n    # Title Extraction\n    title_tag = soup.find('title')\n    title = title_tag.get_text() if title_tag else \"\"\n\n    # Script and Style Tag Removal\n    for tag in soup.find_all(['script', 'style']):\n        tag.extract()\n\n    # Links extraction\n    links = soup.find_all('a')\n    link_urls = []\n    for link in links:\n        if 'href' in link.attrs:\n            link_urls.append(urljoin(base_url, link['href']))\n\n    # Images extraction\n    images = soup.find_all('img')\n    image_urls = []\n    for image in images:\n        if 'src' in image.attrs:\n            # if http or https is not present in the image url, join it with the base url\n            if 'http' not in image['src']:\n                image_urls.append(urljoin(base_url, image['src']))\n            else:\n                image_urls.append(image['src'])\n\n    # Body Extraction (if it exists)\n    body_content = soup.find('body')\n    if body_content:\n        # Minify the HTML within the body tag\n        minimized_body = minify(str(body_content))\n\n        return title, minimized_body, link_urls, image_urls\n        # return \"Title: \" + title + \", Body: \" + minimized_body + \", Links: \" + str(link_urls) + \", Images: \" + str(image_urls)\n\n    # throw an error if no body content is found\n    raise ValueError(\"No HTML body content found, please try setting the 'headless' flag to False in the graph configuration.\")\n```"
    },
    {
        "prompt": "Show me the implementation of the research_web function in ScrapeGraphAI.",
        "answer": "```python\n\"\"\"\nModule for making the request on the web\n\"\"\"\nimport re\nfrom typing import List\nfrom langchain_community.tools import DuckDuckGoSearchResults\nfrom googlesearch import search as google_search\n\n\ndef search_on_web(query: str, search_engine: str = \"Google\", max_results: int = 10) -> List[str]:\n    \"\"\"\n    Searches the web for a given query using specified search engine options.\n\n    Args:\n        query (str): The search query to find on the internet.\n        search_engine (str, optional): Specifies the search engine to use, options include 'Google' or 'DuckDuckGo'. Default is 'Google'.\n        max_results (int, optional): The maximum number of search results to return.\n\n    Returns:\n        List[str]: A list of URLs as strings that are the search results.\n\n    Raises:\n        ValueError: If the search engine specified is neither 'Google' nor 'DuckDuckGo'.\n\n    Example:\n        >>> search_on_web(\"example query\", search_engine=\"Google\", max_results=5)\n        ['http://example.com', 'http://example.org', ...]\n\n    This function allows switching between Google and DuckDuckGo to perform internet searches, returning a list of result URLs.\n    \"\"\"\n\n    if search_engine.lower() == \"google\":\n        res = []\n\n        for url in google_search(query, stop=max_results):\n            res.append(url)\n        return res\n    elif search_engine.lower() == \"duckduckgo\":\n        research = DuckDuckGoSearchResults(max_results=max_results)\n        res = research.run(query)\n\n        links = re.findall(r'https?://[^\\s,\\]]+', res)\n\n        return links\n    raise ValueError(\n        \"The only search engines available are DuckDuckGo or Google\")\n\n```"
    },
    {
        "prompt": "Show me the implementation of the sys_dynamic_import function in ScrapeGraphAI.",
        "answer": "```python\n\"\"\"high-level module for dynamic importing of python modules at runtime\n\nsource code inspired by https://gist.github.com/DiTo97/46f4b733396b8d7a8f1d4d22db902cfc\n\"\"\"\n\nimport sys\nimport typing\n\n\nif typing.TYPE_CHECKING:\n    import types\n\n\ndef srcfile_import(modpath: str, modname: str) -> \"types.ModuleType\":\n    \"\"\"imports a python module from its srcfile\n\n    Args:\n        modpath: The srcfile absolute path\n        modname: The module name in the scope\n\n    Returns:\n        The imported module\n\n    Raises:\n        ImportError: If the module cannot be imported from the srcfile\n    \"\"\"\n    import importlib.util  # noqa: F401\n\n    #\n    spec = importlib.util.spec_from_file_location(modname, modpath)\n\n    if spec is None:\n        message = f\"missing spec for module at {modpath}\"\n        raise ImportError(message)\n\n    if spec.loader is None:\n        message = f\"missing spec loader for module at {modpath}\"\n        raise ImportError(message)\n\n    module = importlib.util.module_from_spec(spec)\n\n    # adds the module to the global scope\n    sys.modules[modname] = module\n\n    spec.loader.exec_module(module)\n\n    return module\n\n\ndef dynamic_import(modname: str, message: str = \"\") -> None:\n    \"\"\"imports a python module at runtime\n\n    Args:\n        modname: The module name in the scope\n        message: The display message in case of error\n\n    Raises:\n        ImportError: If the module cannot be imported at runtime\n    \"\"\"\n    if modname not in sys.modules:\n        try:\n            import importlib  # noqa: F401\n\n            module = importlib.import_module(modname)\n            sys.modules[modname] = module\n        except ImportError as x:\n            raise ImportError(message) from x\n\n```"
    }
]